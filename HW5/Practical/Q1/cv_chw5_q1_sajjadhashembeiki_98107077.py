# -*- coding: utf-8 -*-
"""CV_CHW5_Q1_SajjadHashembeiki_98107077.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xGTR1eZeYdhemYCVsNKH17Oe0510Ig_T

# Part1
"""

# Import libraries
import tqdm
import torch
import torchvision

import numpy as np
import torch.nn as nn
import seaborn as sns
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torchvision.transforms as transforms

from sklearn.metrics import confusion_matrix

# Define the transformation
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# Load CIFAR-10
train_set = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform)

test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform)


# Split the dataset into train, validation sests
train_size = 40000
val_size = 10000

train_dataset, val_dataset = torch.utils.data.random_split(
    train_set, [train_size, val_size])


# Create data loaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True)

val_loader = torch.utils.data.DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False)

test_loader = torch.utils.data.DataLoader(
    test_dataset, batch_size=batch_size, shuffle=False)

# Function to show a random image from each class
def plot_images(loader, dataset):

    # Get the classes names
    classes = dataset.classes

    # Get the a batch of the loader
    images, labels = next(iter(loader))

    # Plot one random image from each class
    fig, axs = plt.subplots(2, 5, figsize=(10, 5))
    fig.suptitle('Random Images from Each Class')

    for i in range(10):
        class_indices = (labels == i).nonzero().flatten()
        random_index = class_indices[np.random.randint(
            len(class_indices))]

        image = images[random_index].numpy()
        image = np.transpose(image, (1, 2, 0))
        mean = np.array([0.5, 0.5, 0.5])
        std = np.array([0.5, 0.5, 0.5])
        image = std * image + mean
        image = np.clip(image, 0, 1)

        axs[i // 5, i % 5].imshow(image)
        axs[i // 5, i % 5].set_title(classes[i])
        axs[i // 5, i % 5].axis('off')

    plt.show()

# Plot random images from each class
plot_images(train_loader, train_set)

"""# Part 2"""

# Train function
def train(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    for inputs, labels in tqdm.tqdm(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_samples += labels.size(0)

    accuracy = correct_predictions / total_samples

    return running_loss / len(train_loader), accuracy

# Validation function
def validate(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

    validation_loss = running_loss / len(val_loader)
    accuracy = correct_predictions / total_samples

    return validation_loss, accuracy

# Test function
def test(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

    test_loss = running_loss / len(test_loader)
    accuracy = correct_predictions / total_samples

    return test_loss, accuracy

class BaselineModel(nn.Module):
      def __init__(self):
          super(BaselineModel, self).__init__()
          self.conv1 = nn.Conv2d(
              in_channels=3, out_channels=32, kernel_size=5)
          self.relu = nn.ReLU()
          self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
          self.flatten = nn.Flatten()
          self.fc = nn.Linear(32 * 14 * 14, 10)

      def forward(self, x):
          x = self.conv1(x)
          x = self.relu(x)
          x = self.maxpool(x)
          x = self.flatten(x)
          x = self.fc(x)
          return x

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model, loss function, and optimizer
baseline_model = BaselineModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)

# Set worst loss for save the best model
best_val_loss = float('inf')
best_epoch = -1

# Save per epochs stats for plot
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# Set number of epochs
num_epochs = 20

# Loop through the epochs
for epoch in range(num_epochs):

    # train the model
    train_loss, train_accuracy = train(
        baseline_model, train_loader, criterion, optimizer, device)

    # validate the model
    val_loss, val_accuracy = validate(
        baseline_model, val_loader, criterion, device)

    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    print(f"Epoch {epoch + 1}/{num_epochs} || "
          f"Train Loss: {train_loss:.4f}, "
          f"Train Accuracy: {train_accuracy * 100:.2f}%, "
          f"Validation Loss: {val_loss:.4f}, "
          f"Validation Accuracy: {val_accuracy * 100:.2f}%")

    # Save the best model based on validation loss
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_epoch = epoch
        best_model_state = baseline_model.state_dict()

# Save the best model
torch.save(best_model_state, 'best_baseline_model.pth')
print(f'Best model saved in epoch {best_epoch + 1}')

# Save loss and accuracy values in train_history dict for plot
train_history = {
    'train_loss': train_losses, 'train_accuracy': train_accuracies,
    'val_loss': val_losses, 'val_accuracy': val_accuracies}

# A function for plot the loss and accs for each epoch
def plot_training_history(train_history):
    epochs = range(1, len(train_history['train_loss']) + 1)
    # Plot Loss
    plt.figure(figsize=(8, 3))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_history['train_loss'], label='Training Loss')
    plt.plot(epochs, train_history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_history['train_accuracy'],
             label='Training Accuracy')

    plt.plot(epochs, train_history['val_accuracy'],
             label='Validation Accuracy')

    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.tight_layout()
    plt.show()

# Apply the function and plot the history
plot_training_history(train_history)

#Load the best model
best_model = BaselineModel()
best_model.load_state_dict(torch.load('best_baseline_model.pth'))
best_model.to(device)

# Test the best model on the testset
test_loss, test_accuracy = test(
    best_model, test_loader, criterion, device)

# Print the test performances
print(f"Test Loss: {test_loss:.4f},"
      f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""##### Part2 Summary: In this part I trained the baseline model for 20 epochs. The best model is selected based on validation loss (in 6th epoch). The best model performance is mentioned below.
 -  Train Accuracy: 70.01%
 -  Validaton Accuracy: 65.31%
 -  Test Accuracy: 63.32%

 -  Train Loss: 0.8773
 -  Validation Loss: 1.0232
 - Test Loss: 1.2171

##### The best model is saved as "best_baseline_model.pth"

# Part3
"""

# Improve the baseline model

class ThirdPartModel(nn.Module):
      def __init__(self):
          super(ThirdPartModel, self).__init__()
          self.conv1 = nn.Conv2d(
              in_channels=3, out_channels=32, kernel_size=5)
          self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
          self.conv3 = nn.Conv2d(64, 128, kernel_size=3)
          self.relu = nn.ReLU()
          self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
          self.flatten = nn.Flatten()
          self.fc1 = nn.Linear(128*2*2, 256)
          self.fc2 = nn.Linear(256, 10)

      def forward(self, x):
          x = self.conv1(x)
          x = self.relu(x)
          x = self.maxpool(x)

          x = self.conv2(x)
          x = self.relu(x)
          x = self.maxpool(x)

          x = self.conv3(x)
          x = self.relu(x)
          x = self.maxpool(x)

          x = self.flatten(x)
          x = self.fc1(x)
          x = self.relu(x)
          x = self.fc2(x)

          return x

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model, loss function, and optimizer
third_part_model = ThirdPartModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(third_part_model.parameters(), lr=0.001)

# Set worst loss for save the best model
best_val_loss = float('inf')
best_epoch = -1

# Save per epochs stats for plot
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# Set number of epochs
num_epochs = 20

# Loop through the epochs
for epoch in range(num_epochs):

    # train the model
    train_loss, train_accuracy = train(
        third_part_model, train_loader, criterion, optimizer, device)

    # validate the model
    val_loss, val_accuracy = validate(
        third_part_model, val_loader, criterion, device)

    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Print stats
    print(f"Epoch {epoch + 1}/{num_epochs} || "
          f"Train Loss: {train_loss:.4f}, "
          f"Train Accuracy: {train_accuracy * 100:.2f}%, "
          f"Validation Loss: {val_loss:.4f}, "
          f"Validation Accuracy: {val_accuracy * 100:.2f}%")

    # Save the best model based on validation loss
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_epoch = epoch
        best_model_state = third_part_model.state_dict()

# Save the best model
torch.save(best_model_state, 'best_3rd_part_model.pth')
print(f'Best model saved in epoch {best_epoch + 1}')

# Save loss and accuracy values in train_history dict for plot
train_history = {
    'train_loss': train_losses, 'train_accuracy': train_accuracies,
    'val_loss': val_losses, 'val_accuracy': val_accuracies}

# Apply the function and plot the history
plot_training_history(train_history)

#Load the best model
best_3rd_model = ThirdPartModel()
best_3rd_model.load_state_dict(torch.load('best_3rd_part_model.pth'))
best_3rd_model.to(device)

# Test the best model on the testset
test_loss, test_accuracy = test(
    best_3rd_model, test_loader, criterion, device)

# Print the test performances
print(f"Test Loss: {test_loss:.4f},"
      f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""##### Part3 Summary: In this part I trained the ThirdPart model for 20 epochs. The best model is selected based on validation loss (in 7th epoch). The best model performance is mentioned below.
 -  Train Accuracy: 79.47%
 -  Validaton Accuracy: 71.98%
 -  Test Accuracy: 70.17%

 -  Train Loss: 0.5794
 -  Validation Loss: 0.8243
 -  Test Loss: 1.6730

##### The best model is saved as "best_3rd_part_model.pth"

##### In this part I added more Conv and FC layers. Acctually I used 3 Conv Block. Each Conv block consists of a Conv layer,ReLU activation function, and MaxPooling layer.Also I added a FC layer with ReLU activation function with 256 neurons. This model has more complexity than the basemodel and can extract better features and information from the data.

##### As you see the model performance is better than the baseline model in previous part, from 63.32% to 70.17% in test accuracy.

# Part4
"""

# Improve the Third part model

class ForthPartModel(nn.Module):
    def __init__(self):
        super(ForthPartModel, self).__init__()
        self.conv1 = nn.Conv2d(
            in_channels=3, out_channels=32, kernel_size=5)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)
        self.bn3 = nn.BatchNorm2d(128)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(128 * 2 * 2, 256)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)

        return x

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model, loss function, and optimizer
forth_part_model = ForthPartModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(forth_part_model.parameters(), lr=0.001)

# Set worst loss for save the best model
best_val_loss = float('inf')
best_epoch = -1

# Save per epochs stats for plot
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# Set number of epochs
num_epochs = 20

# Loop through the epochs
for epoch in range(num_epochs):

    # train the model
    train_loss, train_accuracy = train(
        forth_part_model, train_loader, criterion, optimizer, device)

    # validate the model
    val_loss, val_accuracy = validate(
        forth_part_model, val_loader, criterion, device)

    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Print stats
    print(f"Epoch {epoch + 1}/{num_epochs} || "
          f"Train Loss: {train_loss:.4f}, "
          f"Train Accuracy: {train_accuracy * 100:.2f}%, "
          f"Validation Loss: {val_loss:.4f}, "
          f"Validation Accuracy: {val_accuracy * 100:.2f}%")

    # Save the best model based on validation loss
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_epoch = epoch
        best_model_state = forth_part_model.state_dict()

# Save the best model
torch.save(best_model_state, 'best_4th_part_model.pth')
print(f'Best model saved in epoch {best_epoch + 1}')

# Save loss and accuracy values in train_history dict for plot
train_history = {
    'train_loss': train_losses, 'train_accuracy': train_accuracies,
    'val_loss': val_losses, 'val_accuracy': val_accuracies}

# Apply the function and plot the history
plot_training_history(train_history)

#Load the best model
best_4th_model = ForthPartModel()
best_4th_model.load_state_dict(torch.load('best_4th_part_model.pth'))
best_4th_model.to(device)

# Test the best model on the testset
test_loss, test_accuracy = test(
    best_4th_model, test_loader, criterion, device)

# Print the test performances
print(f"Test Loss: {test_loss:.4f},"
      f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""##### Part4 Summary: In this part I trained the ForthPart model for 20 epochs. The best model is selected based on validation loss (in 6th epoch). The best model performance is mentioned below.
 -  Train Accuracy: 81.92%
 -  Validaton Accuracy: 74.20%
 -   Test Accuracy: 73.94%

 - Train Loss: 0.5144
 -  Validation Loss: 0.7562
 -  Test Loss: 1.2597

##### The best model is saved as "best_4th_part_model.pth"

##### In this part I added BN layer after each Conv block. BN is a regularization method so can prevent overfitting, and also accelerate convergence during training. BN normalizes the input to a layer by subtracting the batch mean and dividing by the batch standard deviation. This is done independently for each feature in the input.The normalized values are then scaled and shifted by learnable parameters (gamma and beta). This allows the model to adapt the normalization to the specific needs of the layer.


##### As you see the model performance is better than the previous part, about 4% improvment in test accuray (from 70.17% to 73.94%).

# Part5
"""

# Improve the Forth Part model
class FifthPartModel(nn.Module):
    def __init__(self):
        super(FifthPartModel, self).__init__()
        self.conv1 = nn.Conv2d(
            in_channels=3, out_channels=32, kernel_size=5)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)
        self.bn3 = nn.BatchNorm2d(128)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(128 * 2 * 2, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)

        return x

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model, loss function, and optimizer
fifth_part_model = FifthPartModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(fifth_part_model.parameters(), lr=0.001)

# Set worst loss for save the best model
best_val_loss = float('inf')
best_epoch = -1

# Save per epochs stats for plot
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# Set number of epochs
num_epochs = 20

# Loop through the epochs
for epoch in range(num_epochs):

    # train the model
    train_loss, train_accuracy = train(
        fifth_part_model, train_loader, criterion, optimizer, device)

    # validate the model
    val_loss, val_accuracy = validate(
        fifth_part_model, val_loader, criterion, device)

    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Print stats
    print(f"Epoch {epoch + 1}/{num_epochs} || "
          f"Train Loss: {train_loss:.4f}, "
          f"Train Accuracy: {train_accuracy * 100:.2f}%, "
          f"Validation Loss: {val_loss:.4f}, "
          f"Validation Accuracy: {val_accuracy * 100:.2f}%")

    # Save the best model based on validation loss
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_epoch = epoch
        best_model_state = fifth_part_model.state_dict()

# Save the best model
torch.save(best_model_state, 'best_5th_part_model.pth')
print(f'Best model saved in epoch {best_epoch + 1}')

# Save loss and accuracy values in train_history dict for plot
train_history = {
    'train_loss': train_losses, 'train_accuracy': train_accuracies,
    'val_loss': val_losses, 'val_accuracy': val_accuracies}

# Apply the function and plot the history
plot_training_history(train_history)

#Load the best model
best_5th_model = FifthPartModel()
best_5th_model.load_state_dict(torch.load('best_5th_part_model.pth'))
best_5th_model.to(device)

# Test the best model on the testset
test_loss, test_accuracy = test(
    best_5th_model, test_loader, criterion, device)

# Print the test performances
print(f"Test Loss: {test_loss:.4f},"
      f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""##### Part5 Summary: In this part I trained the FifthPart model for 20 epochs. The best model is selected based on validation loss (in _th epoch). The best model performance is mentioned below.
 -  Train Accuracy: 79.14%
 -  Validaton Accuracy: 75.46%
 - Test Accuracy: 75.79%

 - Train Loss: 0.5974
 -  Validation Loss: 0.7218
 -  Test Loss: 0.9023

##### The best model is saved as "best_5th_part_model.pth"

##### In this part I add Dropout before the last FC layer. Dropout is a regularization method that helps us to get rid of overfittng. Dropout randomly turns off (setting to zero) a proportion of the neurons during training. This prevents the model from becoming too reliant on specific neurons.

##### This model has highest test accuracy and lowest test loss between all the models in this assignment. The Dropout layer did its work so nicely and improve generalization of the model.

# Part6

### In this part I want to compare the models all togather. First of all I have ran the best model of each part on the testset (1000images) in the previous parts and here I want to just report the whole results and compare the models togather, so lets get starded!
#### In the first part with the base line model I got 63.32% test accuracy, in the 3rd part I added some Conv blocks and a FC layer to make the model more complex and I got 70.17% test accuracy in this part. In 4th and 5th parts I just add BN and Dropout respectively. In 4th part the model test accuracy reached to 73.94% and in 5th part reached to 75.79%. As you see the model performance is increasing.

# Part6 - Confusion Matrix
"""

# Load the best model of each part

# Best baseline model
best_baseline_model = BaselineModel()
best_baseline_model.load_state_dict(
    torch.load('best_baseline_model.pth'))
best_baseline_model.to(device)

# Best 3th-part model
best_3rd_model = ThirdPartModel()
best_3rd_model.load_state_dict(torch.load('best_3rd_part_model.pth'))
best_3rd_model.to(device)

# Best 4th-part model
best_4th_model = ForthPartModel()
best_4th_model.load_state_dict(torch.load('best_4th_part_model.pth'))
best_4th_model.to(device)

# Best 5th-part model
best_5th_model = FifthPartModel()
best_5th_model.load_state_dict(torch.load('best_5th_part_model.pth'))
best_5th_model.to(device)

# Function to get predictions for the entire test dataset
def get_predictions(model, dataloader, device):
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return all_preds, all_labels

# Define class names for label of plot
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog',
               'frog', 'horse', 'ship', 'truck']

# A function for calculate and plot confusion matrix
def conf_mat(labels, preds, modelName):

    # Create the confusion matrix
    conf_matrix = confusion_matrix(labels, preds)

    # Plot the confusion matrix
    plt.figure(figsize=(5, 4))
    sns.heatmap(
        conf_matrix, annot=True, fmt='d', cmap='Blues',
        xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title(f'Confusion Matrix {modelName}')
    plt.show()

# Get predictions for the entire test dataset
# Base model
preds_base, labels_base = get_predictions(
    best_baseline_model, test_loader, device)

# 3rd model
preds_3rd, labels_3rd = get_predictions(
    best_3rd_model, test_loader, device)

# 4th model
preds_4th, labels_4th = get_predictions(
    best_4th_model, test_loader, device)

# 5th model
preds_5th, labels_5th = get_predictions(
    best_5th_model, test_loader, device)


# Calculate and plot the confusion matrix
conf_mat(labels_base, preds_base, 'Best Baseline Model')
conf_mat(labels_3rd, preds_3rd, 'Best 3rd Part Model')
conf_mat(labels_4th, preds_4th, 'Best 4th Part Model')
conf_mat(labels_5th, preds_5th, 'Best 5th Part Model')